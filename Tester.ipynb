{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48a0f212-15b6-420c-9baf-ba98400f48d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tensordict import TensorDict, TensorDictBase\n",
    "from torchrl.envs import EnvBase\n",
    "from torchrl.data.tensor_specs import OneHot, Composite, BoundedTensorSpec, Categorical, Unbounded, Binary\n",
    "import itertools\n",
    "from localcider.sequenceParameters import SequenceParameters\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "import math\n",
    "\n",
    "from torchrl.envs import TransformedEnv, FlattenObservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2149ec13-ff6b-46fb-bae8-3344a7af5c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwapVar(EnvBase): #dtype of tensors?\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reference_state = 'MASASSSQRGRSGSGNFGGGRGGGFGGNDNFGRGGNFSGRGGFGGSRGGGGYGGSGDGYNGFGNDGSNFGGGGSYNDFGNYNNQSSNFGPMKGGNFGGRSSGGSGGGGQYFAKPRNQGGYGGSSSSSSYGSGRRF'\n",
    "        self.seq_len = len(self.reference_state)\n",
    "        self.alphabet = list(set(self.reference_state))\n",
    "        self.initial_state = 'X'*self.seq_len\n",
    "        self.alphabet.append('X')\n",
    "        self.aa_to_idx = {aa: idx for idx, aa in enumerate(self.alphabet)}\n",
    "        self.idx_to_aa = {idx: aa for idx, aa in enumerate(self.alphabet)}\n",
    "\n",
    "        self.target = 0.8\n",
    "        \n",
    "        self._batch_size = torch.Size()\n",
    "        \n",
    "        #observation_spec = OneHot(n=len(self.alphabet),shape=(self.seq_len,len(self.alphabet)),dtype=torch.float,device='cpu')\n",
    "        observation_spec = Binary(n=self.seq_len*len(self.alphabet),dtype=torch.float)\n",
    "        self.observation_spec = Composite(observation=observation_spec)\n",
    "\n",
    "        self.action_spec = Categorical(len(self.alphabet)-1,shape=torch.Size([1]),dtype=torch.int64)\n",
    "        \n",
    "        self.reward_spec = Unbounded(shape=torch.Size([1]))\n",
    "\n",
    "    def fwd_onehot(self, sequence):\n",
    "        indices = [self.aa_to_idx[aa] for aa in sequence]\n",
    "        tensor_indices = torch.tensor(indices)\n",
    "        one_hot = torch.nn.functional.one_hot(tensor_indices, num_classes=len(self.alphabet)).to(torch.float)\n",
    "        return one_hot.flatten()\n",
    "\n",
    "    def bwd_onehot(self, onehot):\n",
    "        onehot = torch.reshape( onehot, (int(len(onehot)/len(self.alphabet)),len(self.alphabet)) )\n",
    "        indices = torch.argmax(onehot, dim=1).tolist()\n",
    "        sequence = ''.join([self.idx_to_aa[idx] for idx in indices])\n",
    "        return sequence\n",
    "\n",
    "    def reward(self, sequence):\n",
    "        sequence = sequence[:self.n_step]\n",
    "        if len(sequence) < self.seq_len:\n",
    "            return 0, False\n",
    "        else:\n",
    "            SeqOb = SequenceParameters(sequence)\n",
    "            score = SeqOb.get_kappa()\n",
    "            return -abs(score-self.target), True\n",
    "\n",
    "    def _reset(self, tensordict):\n",
    "        if tensordict is not None:\n",
    "            action = tensordict.get('action')\n",
    "        else:\n",
    "            action = self.action_spec.rand()\n",
    "        #action = self.action_spec.rand()\n",
    "        self.n_step = 0\n",
    "        observation = self.fwd_onehot(self.initial_state)\n",
    "        out_td = TensorDict({'observation': observation, 'action': action},batch_size=torch.Size())\n",
    "        return out_td\n",
    "\n",
    "    def _step(self, tensordict: TensorDictBase):\n",
    "        #print(self.n_step)\n",
    "        action = tensordict.get('action')\n",
    "        m = self.alphabet[action]\n",
    "        #print(m)\n",
    "        state = list( self.bwd_onehot(tensordict.get('observation').clone()) )\n",
    "        state[self.n_step] = m\n",
    "        #print(state)\n",
    "        state = ''.join(state)\n",
    "        #print(state)\n",
    "        self.n_step += 1\n",
    "        reward, done = self.reward(state)\n",
    "\n",
    "        next_observation = self.fwd_onehot(state)\n",
    "        \n",
    "        td = TensorDict(\n",
    "            {\n",
    "                'observation': next_observation,\n",
    "                'reward': torch.tensor([reward]).float(),\n",
    "                'done': torch.tensor([done]).bool()\n",
    "            },\n",
    "            batch_size=torch.Size()\n",
    "        )\n",
    "        return td\n",
    "\n",
    "    def _set_seed(self, seed: int):\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ade330d9-7659-4f6f-9a25-3aefe40b28f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SwapVar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21c7acbf-13fa-41a7-b2e8-396159860750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        next: TensorDict(\n",
       "            fields={\n",
       "                done: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                observation: Tensor(shape=torch.Size([5, 1755]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                reward: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                terminated: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "            batch_size=torch.Size([5]),\n",
       "            device=None,\n",
       "            is_shared=False),\n",
       "        observation: Tensor(shape=torch.Size([5, 1755]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([5]),\n",
       "    device=None,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.rollout(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4b531180-c572-495c-bd6a-b6f0f11f3781",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-25 18:43:34,488 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    }
   ],
   "source": [
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da99887c-a566-493b-8cb3-8dc2d18dd972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "from tensordict.nn import TensorDictModule\n",
    "from torchrl.modules import ProbabilisticActor, TanhNormal, ValueOperator\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.objectives.value import GAE\n",
    "from torchrl.objectives import ClipPPOLoss\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27166992-b392-457e-a567-4c0c0c4cd9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cells = 256  # number of cells in each layer i.e. output dim.\n",
    "lr = 3e-4\n",
    "max_grad_norm = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad50acb0-a012-4b27-9fd1-a2e4ecf280a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_per_batch = 1000\n",
    "# For a complete training, bring the number of frames up to 1M\n",
    "total_frames = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06ba2bd5-856c-4a94-a59a-ae39e6240cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_batch_size = 64  # cardinality of the sub-samples gathered from the current data in the inner loop\n",
    "num_epochs = 10  # optimisation steps per batch of data collected\n",
    "clip_epsilon = (\n",
    "    0.2  # clip value for PPO loss: see the equation in the intro for more context.\n",
    ")\n",
    "gamma = 0.99\n",
    "lmbda = 0.95\n",
    "entropy_eps = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c408455-9ae7-43e2-9451-0b100797b276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running policy: TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        loc: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([1755]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        scale: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "actor_net = nn.Sequential(\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(2*env.action_spec.shape[-1], device=device),\n",
    "    NormalParamExtractor(),\n",
    ")\n",
    "\n",
    "policy_module = TensorDictModule(\n",
    "    actor_net, in_keys=[\"observation\"], out_keys=[\"loc\", \"scale\"]\n",
    ")\n",
    "\n",
    "actor = ProbabilisticActor(\n",
    "    module=policy_module,\n",
    "    spec=env.action_spec,\n",
    "    in_keys=[\"loc\", \"scale\"],\n",
    "    distribution_class=TanhNormal,\n",
    "    distribution_kwargs={\"min\": 0.0, \"max\": 1.0},\n",
    "    return_log_prob=True,\n",
    ")\n",
    "\n",
    "print(\"Running policy:\", policy_module(env.reset()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0484ddb7-7e0d-4bb4-b445-3fe2255e4d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running value: TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([1755]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        state_value: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "value_net = nn.Sequential(\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(1, device=device),\n",
    ")\n",
    "\n",
    "value_module = ValueOperator(\n",
    "    module=value_net,\n",
    "    in_keys=[\"observation\"],\n",
    ")\n",
    "\n",
    "print(\"Running value:\", value_module(env.reset()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e552db59-b7ae-4a60-95a5-76998db0c204",
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    policy_module,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=total_frames,\n",
    "    split_trajs=False,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a75d5fb4-244d-4f1f-afde-0fe122e11f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(\n",
    "    storage=LazyTensorStorage(max_size=frames_per_batch),\n",
    "    sampler=SamplerWithoutReplacement(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "751254be-b221-4ee3-a51e-42f6faca6645",
   "metadata": {},
   "outputs": [],
   "source": [
    "advantage_module = GAE(\n",
    "    gamma=gamma, lmbda=lmbda, value_network=value_module, average_gae=True\n",
    ")\n",
    "\n",
    "loss_module = ClipPPOLoss(\n",
    "    actor_network=policy_module,\n",
    "    critic_network=value_module,\n",
    "    clip_epsilon=clip_epsilon,\n",
    "    entropy_bonus=bool(entropy_eps),\n",
    "    entropy_coef=entropy_eps,\n",
    "    # these keys match by default but we set this for completeness\n",
    "    critic_coef=1.0,\n",
    "    loss_critic_type=\"smooth_l1\",\n",
    ")\n",
    "\n",
    "optim = torch.optim.Adam(loss_module.parameters(), lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optim, total_frames // frames_per_batch, 0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2af17832-8170-4880-9cd0-8250a4028f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                             | 0/10000 [18:33<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'get_dist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(frames_per_batch \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m sub_batch_size):\n\u001b[1;32m     17\u001b[0m     subdata \u001b[38;5;241m=\u001b[39m replay_buffer\u001b[38;5;241m.\u001b[39msample(sub_batch_size)\n\u001b[0;32m---> 18\u001b[0m     loss_vals \u001b[38;5;241m=\u001b[39m \u001b[43mloss_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     loss_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     20\u001b[0m         loss_vals[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_objective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;241m+\u001b[39m loss_vals[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_critic\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;241m+\u001b[39m loss_vals[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_entropy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     23\u001b[0m     )\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Optimization: backward, grad clipping and optimization step\u001b[39;00m\n",
      "File \u001b[0;32m/shared/fpesce/miniconda3/envs/PeptidyneRL/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/shared/fpesce/miniconda3/envs/PeptidyneRL/lib/python3.12/site-packages/torch/nn/modules/module.py:1844\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1843\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1844\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1846\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1849\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/shared/fpesce/miniconda3/envs/PeptidyneRL/lib/python3.12/site-packages/torch/nn/modules/module.py:1790\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1787\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1788\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1790\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1792\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1793\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1794\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1795\u001b[0m     ):\n\u001b[1;32m   1796\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/shared/fpesce/miniconda3/envs/PeptidyneRL/lib/python3.12/site-packages/torchrl/objectives/common.py:49\u001b[0m, in \u001b[0;36m_forward_wrapper.<locals>.new_forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_exploration_type(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeterministic_sampling_mode):\n\u001b[0;32m---> 49\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/shared/fpesce/miniconda3/envs/PeptidyneRL/lib/python3.12/site-packages/tensordict/nn/common.py:314\u001b[0m, in \u001b[0;36mdispatch.__call__.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m out\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _self \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(tensordict, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/shared/fpesce/miniconda3/envs/PeptidyneRL/lib/python3.12/site-packages/torchrl/objectives/ppo.py:860\u001b[0m, in \u001b[0;36mClipPPOLoss.forward\u001b[0;34m(self, tensordict)\u001b[0m\n\u001b[1;32m    857\u001b[0m     scale \u001b[38;5;241m=\u001b[39m advantage\u001b[38;5;241m.\u001b[39mstd()\u001b[38;5;241m.\u001b[39mclamp_min(\u001b[38;5;241m1e-6\u001b[39m)\n\u001b[1;32m    858\u001b[0m     advantage \u001b[38;5;241m=\u001b[39m (advantage \u001b[38;5;241m-\u001b[39m loc) \u001b[38;5;241m/\u001b[39m scale\n\u001b[0;32m--> 860\u001b[0m log_weight, dist, kl_approx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;66;03m# ESS for logging\u001b[39;00m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;66;03m# In theory, ESS should be computed on particles sampled from the same source. Here we sample according\u001b[39;00m\n\u001b[1;32m    864\u001b[0m     \u001b[38;5;66;03m# to different, unrelated trajectories, which is not standard. Still it can give a idea of the dispersion\u001b[39;00m\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;66;03m# of the weights.\u001b[39;00m\n",
      "File \u001b[0;32m/shared/fpesce/miniconda3/envs/PeptidyneRL/lib/python3.12/site-packages/torchrl/objectives/ppo.py:483\u001b[0m, in \u001b[0;36mPPOLoss._log_weight\u001b[0;34m(self, tensordict)\u001b[0m\n\u001b[1;32m    478\u001b[0m action \u001b[38;5;241m=\u001b[39m tensordict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensor_keys\u001b[38;5;241m.\u001b[39maction)\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_network_params\u001b[38;5;241m.\u001b[39mto_module(\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_network\n\u001b[1;32m    482\u001b[0m ) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunctional \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext():\n\u001b[0;32m--> 483\u001b[0m     dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dist\u001b[49m(tensordict)\n\u001b[1;32m    485\u001b[0m prev_log_prob \u001b[38;5;241m=\u001b[39m tensordict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensor_keys\u001b[38;5;241m.\u001b[39msample_log_prob)\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prev_log_prob\u001b[38;5;241m.\u001b[39mrequires_grad:\n",
      "File \u001b[0;32m/shared/fpesce/miniconda3/envs/PeptidyneRL/lib/python3.12/site-packages/tensordict/nn/common.py:1151\u001b[0m, in \u001b[0;36mTensorDictModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1147\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;66;03m# no fallback for private attributes\u001b[39;00m\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodule\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1154\u001b[0m )\n",
      "File \u001b[0;32m/shared/fpesce/miniconda3/envs/PeptidyneRL/lib/python3.12/site-packages/torch/nn/modules/module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1933\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'get_dist'"
     ]
    }
   ],
   "source": [
    "logs = defaultdict(list)\n",
    "pbar = tqdm(total=total_frames)\n",
    "eval_str = \"\"\n",
    "\n",
    "# We iterate over the collector until it reaches the total number of frames it was\n",
    "# designed to collect:\n",
    "for i, tensordict_data in enumerate(collector):\n",
    "    # we now have a batch of data to work with. Let's learn something from it.\n",
    "    for _ in range(num_epochs):\n",
    "        # We'll need an \"advantage\" signal to make PPO work.\n",
    "        # We re-compute it at each epoch as its value depends on the value\n",
    "        # network which is updated in the inner loop.\n",
    "        advantage_module(tensordict_data)\n",
    "        data_view = tensordict_data.reshape(-1)\n",
    "        replay_buffer.extend(data_view.cpu())\n",
    "        for _ in range(frames_per_batch // sub_batch_size):\n",
    "            subdata = replay_buffer.sample(sub_batch_size)\n",
    "            loss_vals = loss_module(subdata.to(device))\n",
    "            loss_value = (\n",
    "                loss_vals[\"loss_objective\"]\n",
    "                + loss_vals[\"loss_critic\"]\n",
    "                + loss_vals[\"loss_entropy\"]\n",
    "            )\n",
    "\n",
    "            # Optimization: backward, grad clipping and optimization step\n",
    "            loss_value.backward()\n",
    "            # this is not strictly mandatory but it's good practice to keep\n",
    "            # your gradient norm bounded\n",
    "            torch.nn.utils.clip_grad_norm_(loss_module.parameters(), max_grad_norm)\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "    logs[\"reward\"].append(tensordict_data[\"next\", \"reward\"].mean().item())\n",
    "    pbar.update(tensordict_data.numel())\n",
    "    cum_reward_str = (\n",
    "        f\"average reward={logs['reward'][-1]: 4.4f} (init={logs['reward'][0]: 4.4f})\"\n",
    "    )\n",
    "    logs[\"step_count\"].append(tensordict_data[\"step_count\"].max().item())\n",
    "    stepcount_str = f\"step count (max): {logs['step_count'][-1]}\"\n",
    "    logs[\"lr\"].append(optim.param_groups[0][\"lr\"])\n",
    "    lr_str = f\"lr policy: {logs['lr'][-1]: 4.4f}\"\n",
    "    if i % 10 == 0:\n",
    "        # We evaluate the policy once every 10 batches of data.\n",
    "        # Evaluation is rather simple: execute the policy without exploration\n",
    "        # (take the expected value of the action distribution) for a given\n",
    "        # number of steps (1000, which is our ``env`` horizon).\n",
    "        # The ``rollout`` method of the ``env`` can take a policy as argument:\n",
    "        # it will then execute this policy at each step.\n",
    "        with set_exploration_type(ExplorationType.MEAN), torch.no_grad():\n",
    "            # execute a rollout with the trained policy\n",
    "            eval_rollout = env.rollout(1000, policy_module)\n",
    "            logs[\"eval reward\"].append(eval_rollout[\"next\", \"reward\"].mean().item())\n",
    "            logs[\"eval reward (sum)\"].append(\n",
    "                eval_rollout[\"next\", \"reward\"].sum().item()\n",
    "            )\n",
    "            logs[\"eval step_count\"].append(eval_rollout[\"step_count\"].max().item())\n",
    "            eval_str = (\n",
    "                f\"eval cumulative reward: {logs['eval reward (sum)'][-1]: 4.4f} \"\n",
    "                f\"(init: {logs['eval reward (sum)'][0]: 4.4f}), \"\n",
    "                f\"eval step-count: {logs['eval step_count'][-1]}\"\n",
    "            )\n",
    "            del eval_rollout\n",
    "    pbar.set_description(\", \".join([eval_str, cum_reward_str, stepcount_str, lr_str]))\n",
    "\n",
    "    # We're also using a learning rate scheduler. Like the gradient clipping,\n",
    "    # this is a nice-to-have but nothing necessary for PPO to work.\n",
    "    scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PeptidyneRL]",
   "language": "python",
   "name": "conda-env-PeptidyneRL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
